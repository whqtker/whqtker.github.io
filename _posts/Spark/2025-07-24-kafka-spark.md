---
title : "[Spark] Kafkaë¥¼ ì‚¬ìš©í•œ Spark Streaming"
date : 2025-07-24 20:15:00 +0900
categories : [Spark]
tags : [spark, ìŠ¤íŒŒí¬, streaming, pyspark, kafka, ì¹´í”„ì¹´]
---

## ðŸ“Œ Apache Kafkaëž€?

`Apache Kafka` ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ëŒ€ê·œëª¨ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ë¶„ì‚° ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° í”Œëž«í¼ì´ë‹¤.

`Pub/Sub` ëª¨ë¸ì„ ë”°ë¥¸ë‹¤. `Producer` ëŠ” ë°ì´í„°ë¥¼ ìƒì‚°í•˜ê³  `Consumer` ëŠ” ë°ì´í„°ë¥¼ ì†Œë¹„í•œë‹¤. ì´ë“¤ì„ ì™„ì „ížˆ ë¶„ë¦¬í•˜ì—¬ ì„œë¡œ ì§ì ‘ì ìœ¼ë¡œ í†µì‹ í•  í•„ìš”ê°€ ì—†ë„ë¡ í•œë‹¤. `Topic` ì€ ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì±„ë„ì´ë‹¤. producerì™€ consumerëŠ” ê°ê° í† í”½ì— ë©”ì‹œì§€ë¥¼ ë°œí–‰í•˜ê³  êµ¬ë…í•œë‹¤.

### íŠ¹ì§•

- ë‹¨ì¼ ì„œë²„ê°€ ì•„ë‹Œ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚°ë˜ì–´ ë™ìž‘ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. kafka í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ê°œë³„ ë…¸ë“œë¥¼ `Broker` ë¼ê³  í•˜ë©°, ì—¬ëŸ¬ ê°œì˜ brokerê°€ ëª¨ì—¬ í•˜ë‚˜ì˜ ì¹´í”„ì¹´ í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì„±í•œë‹¤.
- í•˜ë‚˜ì˜ ê±°ëŒ€í•œ í† í”½ì„ ì—¬ëŸ¬ ê°œì˜ íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„í• í•˜ì—¬, ì´ë“¤ì„ ì—¬ëŸ¬ ë¸Œë¡œì»¤ì— ë¶„ì‚°í•˜ì—¬ ì €ìž¥í•œë‹¤. ë§Œì•½ ë°ì´í„°ì˜ ì–‘ì´ ëŠ˜ì–´ë‚˜ë©´ íŒŒí‹°ì…˜ì˜ ìˆ˜ë¥¼ ëŠ˜ë ¤ ë” ë§Žì€ ë¸Œë¡œì»¤ì— ìž‘ì—…ì„ ë¶„ì‚°ì‹œí‚¬ ìˆ˜ ìžˆë‹¤. consumerëŠ” íŒŒí‹°ì…˜ì— ëŒ€í•˜ì—¬ ë³‘ëŸ´ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ, ë¹ ë¥¸ ì†ë„ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìžˆë‹¤. ë˜í•œ í•˜ë‚˜ì˜ íŒŒí‹°ì…˜ ë‚´ì—ì„œëŠ” ë©”ì‹œì§€ì˜ ìˆœì„œë¥¼ ë³´ìž¥í•œë‹¤.
- ë°ì´í„°ì˜ ìœ ì‹¤ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê° íŒŒí‹°ì…˜ì˜ ë³µì œë³¸ì„ ë‹¤ë¥¸ ë¸Œë¡œì»¤ì— ì €ìž¥í•œë‹¤.
- ìˆ˜ì‹ í•œ ë©”ì‹œì§€ë¥¼ ë©”ëª¨ë¦¬ê°€ ì•„ë‹Œ ë””ìŠ¤í¬ì— ì˜êµ¬ì ìœ¼ë¡œ ì €ìž¥í•œë‹¤. consumerê°€ ë©”ì‹œì§€ë¥¼ ì½ì–´ë„ ë°ì´í„°ë¥¼ ì¦‰ì‹œ ì‚­ì œë˜ì§€ ì•Šê³  ì„¤ì •ëœ ê¸°ê°„ ë™ì•ˆ ë””ìŠ¤í¬ì— ë³´ê´€ëœë‹¤. ì´ëŠ” ìž¥ì•  ë°œìƒ ì‹œ ë°ì´í„° ìœ ì‹¤ì„ ë°©ì§€í•œë‹¤. ì´ ë•Œë¬¸ì— ë‹¤ë¥¸ consumerê°€ ê°™ì€ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë²ˆ ì½ì–´ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤.
- ë°ì´í„°ë² ì´ìŠ¤, ê²€ìƒ‰ ì—”ì§„, ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë“±ê³¼ í†µí•©ì´ ì‰½ë‹¤.
- ìˆ˜í‰ì  í™•ìž¥ì´ ì‰½ë‹¤. í´ëŸ¬ìŠ¤í„°ì˜ ìš©ëŸ‰ì´ ë¶€ì¡±í•´ì§€ë©´ í´ëŸ¬ìŠ¤í„°ì— ë¸Œë¡œì»¤ë¥¼ ì¶”ê°€í•˜ë©´ ëœë‹¤.

---

```python
spark = SparkSession \
    .builder \
    .appName("StructuredWordCount") \
    .config("spark.streaming.stopGracefullyOnShutdown", "true") \
    .config("spark.sql.shuffle.partitions", "3") \
    .getOrCreate()

events = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "quickstart") \
    .option("startingOffsets", "earliest") \
    .load()
```

`spark.sql.shuffle.partitions` ì—ì„œ ë°ì´í„° ì…”í”Œë§ì˜ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤.

`SparkSesion` ì„ ìƒì„±í–ˆë‹¤ë©´ ì´ì œ ì¹´í”„ì¹´ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ì •ì˜í•œë‹¤.

`format` ì„ `kafka` ë¡œ ì§€ì •í•˜ì—¬ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ë§Œë“ ë‹¤.

`kafka.bootstrap.servers` ëŠ” ìŠ¤íŒŒí¬ê°€ ì¹´í”„ì¹´ í´ëŸ¬ìŠ¤í„°ì— ì ‘ì†í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë¸Œë¡œì»¤ ëª©ë¡ì´ë‹¤. ìŠ¤íŒŒí¬ driverì™€ executorëŠ” í•´ë‹¹ ì£¼ì†Œë¥¼ í†µí•´ ë¸Œë¡œì»¤ì™€ í†µì‹ ì„ ì§„í–‰í•œë‹¤.

`subscribe` ëŠ” ì–´ë–¤ ì¹´í”„ì¹´ í† í”½ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ì§€ ì§€ì •í•œë‹¤.

`startingOffsets` ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ê°€ ì‹œìž‘ë  ë•Œ í† í”½ì˜ ì–´ëŠ ì§€ì ë¶€í„° ë°ì´í„°ë¥¼ ì½ì„ì§€ ê²°ì •í•œë‹¤. `earliest` ë¡œ ì„¤ì •í•˜ë©´ í† í”½ì˜ ê°€ìž¥ ì˜¤ëž˜ëœ ë°ì´í„°ë¶€í„° ì½ëŠ”ë‹¤.

```python
schema = StructType([
    StructField("city", StringType()),
    StructField("domain", StringType()),
    StructField("event", StringType())
])

value_df = events.select(
    col('key'),
    from_json(
        col("value").cast("string"), schema
    ).alias("value")
)
```

ì¹´í”„ì¹´ë¥¼ í†µí•´ JSON íŒŒì¼ì„ ì½ê¸° ìœ„í•´ì„œ, ë¨¼ì € JSON ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•´ì•¼ í•œë‹¤. ì´í›„ `from_json` ë©”ì„œë“œë¥¼ í†µí•´ JSON ë¬¸ìžì—´ì„ ì •ì˜í•œ `schema` ì— ë”°ë¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°ì²´ë¥¼ ë§Œë“ ë‹¤.

```python
output_df = concat_df.selectExpr(
                'null',
"""
    to_json(named_struct("lower_city", lower_city, "domain", domain, "behavior", behavior)) as value
""".strip())
```

ì¹´í”„ì¹´ ì‹±í¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ keyì™€ value ë‘ ê°œì˜ ì»¬ëŸ¼ì„ ê¸°ëŒ€í•œë‹¤. `named_struct` ë©”ì„œë“œë¥¼ í†µí•´ ì—¬ëŸ¬ ì»¬ëŸ¼ì„ í•˜ë‚˜ì˜ êµ¬ì¡°ì²´ë¡œ ë¬¶ê³ , `to_json` ë©”ì„œë“œë¥¼ í†µí•´ í•˜ë‚˜ì˜ JSON ë¬¸ìžì—´ë¡œ ì§ë ¬í™”í•œë‹¤.

```python
file_writer = concat_df \
                .writeStream \
                .queryName("transformed json") \
                .format("json") \
                .outputMode("append") \
                .option("path", "transformed") \
                .option("checkpointLocation", "chk/json") \
                .start()

kafka_writer = output_df \
                .writeStream \
                .queryName("transformed kafka") \
                .format("kafka") \
                .option("kafka.bootstrap.servers", "kafka:9092") \
                .option("checkpointLocation", "chk/kafka") \
                .option("topic", "transformed") \
                .outputMode("append") \
                .start()

spark.streams.awaitAnyTermination()
```

ë§Œì•½ ë‘ ê°œ ì´ìƒì˜ ì•„ì›ƒí’‹ì— ë°ì´í„°ë¥¼ ì €ìž¥í•˜ëŠ” ê²½ìš° ìœ„ì™€ ê°™ì´ ë”°ë¡œ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ì •ì˜í•˜ë©´ ëœë‹¤. ë‹¤ë§Œ ì£¼ì˜í•´ì•¼ í•  ì ì€, ë‘ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ëŠ” ë…¼ë¦¬ì ìœ¼ë¡œ ë³„ê°œì˜ ì¿¼ë¦¬ì´ë¯€ë¡œ ë‹¤ë¥¸ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í„°ë¦¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ë§Œì•½ ì´ë¥¼ ê³µìœ í•œë‹¤ë©´ ìƒí˜¸ ê°„ì„­ì´ ë°œìƒí•˜ê²Œ ë˜ë©°, ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ ëœë‹¤.

ë˜í•œ `awaitTermination` ëŒ€ì‹  `awaitAnyTermination` ë¥¼ ì‚¬ìš©í•˜ì˜€ëŠ”ë°, ìŠ¤íŒŒí¬ ì„¸ì…˜ ë‚´ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì¤‘ ì–´ëŠ í•˜ë‚˜ë¼ë„ ì¢…ë£Œí•  ë•Œê¹Œì§€ ëŒ€ê¸°í•˜ëŠ” ë©”ì„œë“œì´ë‹¤. ì´ë¥¼ í†µí•´ ë‘ ê°œ ì´ìƒì˜ ì¿¼ë¦¬ë¥¼ ë™ì‹œë³‘ë ¬ì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìžˆë‹¤.

## ðŸ“Œ Stateless/Stateful transformation

ì—¬ê¸°ì„œ stateëž€ ì´ì „ ë°ì´í„°ì— ëŒ€í•œ ì •ë³´ë‚˜ ì¤‘ê°„ ê³„ì‚° ê²°ê³¼ë¥¼ ë§í•œë‹¤. ìŠ¤íŠ¸ë¦¬ë° ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì´ë¥¼ ê¸°ì–µí•˜ëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ Statelessì™€ Statefulë¡œ ë‚˜ë‰œë‹¤.

`Stateless transformation` ì€ ê° ë°ì´í„°ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì—°ì‚°ì´ë‹¤. í˜„ìž¬ ì²˜ë¦¬ ì¤‘ì¸ ë°ì´í„° ì™¸ ë‹¤ë¥¸ ì •ë³´ëŠ” í•„ìš”í•˜ì§€ ì•Šë‹¤. ìƒíƒœë¥¼ ê´€ë¦¬í•  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì— ê°„ë‹¨í•˜ë©° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ê³  ì²˜ë¦¬ ì†ë„ê°€ ë¹ ë¥´ë‹¤. ê° ë°ì´í„°ê°€ ë…ë¦½ì ì´ë¯€ë¡œ worker ë…¸ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìžˆë‹¤. `select`, `withColumn`, `filter`, `map`, `explode` ì—°ì‚° ë“±ì´ ìžˆë‹¤.

ì „ì²´ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” `Complete` ëª¨ë“œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

`Stateful transformation` ì€ ê³¼ê±° ë°ì´í„°ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ê³  ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜„ìž¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì—°ì‚°ì´ë‹¤. `groupBy().agg()` ì™€ ê°™ì€ ìŠ¤íŠ¸ë¦¬ë° ì§‘ê³„, `window` ë©”ì„œë“œì™€ ê°™ì€ ìœˆë„ìš° ì§‘ê³„, ë‘ ê°œì˜ ìŠ¤íŠ¸ë¦¼ì„ ì¡°ì¸í•˜ëŠ” ì—°ì‚°ì´ ëŒ€í‘œì ì´ë‹¤. ìƒíƒœë¥¼ ì €ìž¥í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ ê³µê°„ì´ í•„ìˆ˜ì ì´ë‹¤. ìƒíƒœ ê´€ë¦¬ë¥¼ ìŠ¤íŒŒí¬ê°€ ìžë™ìœ¼ë¡œ í•  ìˆ˜ ìžˆì§€ë§Œ, ê°œë°œìžê°€ ì§ì ‘ ìƒíƒœë¥¼ ê´€ë¦¬í•  ìˆ˜ë„ ìžˆë‹¤.

---

```python
events = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "pos") \
    .option("startingOffsets", "earliest") \
    .option("failOnDataLoss", "false") \
    .load()

value_df = events.select(
            col('key'),
            from_json(
                col("value").cast("string"), schema).alias("value"))

tf_df = value_df.selectExpr(
            'value.product_id',
            'value.amount')
```

ìœ„ ê³¼ì •ì€ Statelessë¼ê³  ë³¼ ìˆ˜ ìžˆë‹¤. ëª¨ë“  ì—°ì‚°ì€ ê° í–‰ì„ ê¸°ì¤€ìœ¼ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë‹¤ì‹œ ë§í•´, í˜„ìž¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ê³¼ê±°ì˜ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤.

```python
total_df = tf_df.select("product_id", "amount")\
    .groupBy("product_id")\
    .sum("amount").withColumnRenamed("sum(amount)", "total_amount")
```

ë°˜ë©´ ìœ„ ê³¼ì •ì€ Statefulì´ë‹¤. `total_amount` ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ìŠ¤íŒŒí¬ëŠ” ìƒí’ˆì˜ íŒë§¤ì•¡ í•©ê³„ê°€ ì–¼ë§ˆì¸ì§€ ê¸°ì–µí•˜ê³  ìžˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

`groupBy` ê°€ í˜¸ì¶œë˜ë©´ ìŠ¤íŒŒí¬ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê³µê°„ì„ ì¤€ë¹„í•˜ê³ , ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ë©”ëª¨ë¦¬ì—ì„œ ë°ì´í„°ì˜ í‚¤ë¥¼ ì°¾ê³ , ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.

## ðŸ“Œ ìœˆë„ìš° ì§‘ê³„ ì—°ì‚°

í¬ê²Œ ë‘ ê°€ì§€ ìœˆë„ìš° ì¢…ë¥˜ê°€ ì¡´ìž¬í•œë‹¤.

![image.png](assets/img/spark/5.png)

`Tumbling Window`ëŠ” ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ê³ ì •ëœ í¬ê¸°ë¡œ ì„œë¡œ ê²¹ì¹˜ì¹˜ ì•Šê²Œ êµ¬ê°„ì„ ë‚˜ëˆ„ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰, ê° ìœˆë„ìš° ì‚¬ì´ì—ëŠ” ë¹ˆí‹ˆì´ ì—†ë‹¤. ê° ì´ë²¤íŠ¸ëŠ” í•˜ë‚˜ì˜ ìœˆë„ìš°ì—ë§Œ ì†í•œë‹¤.

```python
timestamp_format = "yyyy-MM-dd HH:mm:ss"
tf_df = value_df.select("value.*") \
                .withColumn("create_date", to_timestamp("create_date", timestamp_format))
```

`window` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì‹œê°„ ê¸°ì¤€ì´ ë˜ëŠ” ì»¬ëŸ¼ì€ `Timestamp` íƒ€ìž…ì´ì–´ì•¼ í•œë‹¤.

```python
window_duration = "5 minutes"
window_agg_df = tf_df \
    .groupBy(window(col("create_date"), window_duration)) \
    .sum("amount").withColumnRenamed("sum(amount)", "total_amount")
```

`window` ë©”ì„œë“œë¥¼ í†µí•´ ìœˆë„ìš°ë¥¼ ì •ì˜í•œë‹¤. `col` ë©”ì„œë“œë¥¼ í†µí•´ ì–´ë–¤ ì‹œê°„ ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ í•  ì§€ ì§€ì •í•˜ê³ , `window_duration` ì„ í†µí•´ ìœˆë„ìš°ì˜ í¬ê¸°ë¥¼ ì„¤ì •í•œë‹¤. `sliding_duration` ì„ ì§€ì •í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ìžë™ìœ¼ë¡œ í…€ë¸”ë§ ìœˆë„ìš°ë¡œ ë™ìž‘í•œë‹¤.

![image.png](assets/img/spark/6.png)

`Sliding Window` ëŠ” ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ê³ ì •ëœ í¬ê¸°ë¡œ, ì„œë¡œ ê²¹ì¹˜ë„ë¡ ë‚˜ëˆ„ëŠ” ë°©ì‹ì´ë‹¤. ì¦‰, í•˜ë‚˜ì˜ ì´ë²¤íŠ¸ê°€ ì—¬ëŸ¬ ì›ë„ìš°ì— ë™ì‹œì— í¬í•¨ë  ìˆ˜ ìžˆë‹¤.

ë‘ ê°€ì§€ íŒŒë¼ë¯¸í„°ê°€ ì¡´ìž¬í•œë‹¤. â€˜ìœˆë„ìš° í¬ê¸°â€™ëŠ” ì§‘ê³„ê°€ ìˆ˜í–‰ë  ì „ì²´ êµ¬ê°„ì˜ ê¸¸ì´ì´ë‹¤. â€˜ìŠ¬ë¼ì´ë“œ ê°„ê²©â€™ì€ ìœˆë„ìš°ê°€ ë‹¤ìŒ ìœ„ì¹˜ë¡œ ì´ë™í•˜ëŠ” ì‹œê°„ ê°„ê²©ì´ë‹¤. ë§Œì•½ ìœˆë„ìš° í¬ê¸°ì™€ ìŠ¬ë¼ì´ë“œ ê°„ê²©ì´ ê°™ë‹¤ë©´ ìœˆë„ìš°ëŠ” ê²¹ì¹˜ì§€ ì•Šê²Œ ë˜ë©°, í…€ë¸”ë§ ìœˆë„ìš°ì™€ ë™ì¼í•˜ê²Œ ë™ìž‘í•œë‹¤.

```python
window_duration = "10 minutes"
sliding_duration = "5 minutes"
window_agg_df = tf_df \
    .groupBy(window(col("create_date"), window_duration, sliding_duration)) \
    .sum("amount").withColumnRenamed("sum(amount)", "total_amount")
```

`window` ë©”ì„œë“œì— `sliding_duration` ì„ ì§€ì •í•˜ë©´ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë™ìž‘í•œë‹¤.

## ðŸ“Œ Watermark

`Watermark` ëŠ” ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì—ì„œ ì–¼ë§ˆë‚˜ ëŠ¦ê²Œ ë„ì°©í•˜ëŠ” ë°ì´í„°ê¹Œì§€ ì²˜ë¦¬í•  ê²ƒì¸ê°€ë¥¼ ì •ì˜í•˜ëŠ” ê¸°ì¤€ì´ë‹¤.

ì›Œí„°ë§ˆí¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ì‹œê°„ ê°œë…ì´ ë“±ìž¥í•œë‹¤. `Event Time` ì€ ì‹¤ì œë¡œ ì´ë²¤íŠ¸ê°€ ë°œìƒí•œ ì‹œê°„ì´ë©°, `Processing Time` ì€ ì´ë²¤íŠ¸ê°€ ìŠ¤íŠ¸ë¦¬ë° ì• í”Œë¦¬ì¼€ì´ì…˜ì— ë„ì°©í•˜ì—¬ ì²˜ë¦¬ë˜ëŠ” ì‹œê°„ì´ë‹¤. í•­ìƒ ì´ë²¤íŠ¸ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ê°€ ì •ë ¬ë˜ì–´ ë“¤ì–´ì˜¤ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì´ë‚˜ ì‹œìŠ¤í…œ ë¶€í•˜ ë“± ì—¬ëŸ¬ê°€ì§€ ì´ìœ ê°€ ì¡´ìž¬í•œë‹¤.

ì›Œí„°ë§ˆí¬ëŠ” ì´ë²¤íŠ¸ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë™ìž‘í•œë‹¤. Stateful transformationì—ì„œ ë©”ëª¨ë¦¬ì— ì¤‘ê°„ ì§‘ê³„ ê²°ê³¼ë¥¼ ì €ìž¥í•  ë•Œ, ì´ë¥¼ ì •ë¦¬í•˜ëŠ” ê·œì¹™ì´ ì—†ë‹¤ë©´ stateëŠ” ë¬´í•œížˆ ì»¤ì§€ê²Œ ë˜ë©°, OOM ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.

ë˜í•œ ì›Œí„°ë§ˆí¬ëŠ” ë„ˆë¬´ ëŠ¦ê²Œ ë„ì°©í•˜ëŠ” ë°ì´í„°ë¥¼ ë¬´ì‹œí•˜ëŠ” ê¸°ì¤€ì´ ë˜ê¸°ë„ í•œë‹¤. ì§€ì—° ìž„ê³„ê°’ë³´ë‹¤ ëŠ¦ê²Œ ë„ì°©í•œ ë°ì´í„°ëŠ” ì§‘ê³„ì— í¬í•¨ì‹œí‚¤ì§€ ì•ŠëŠ”ë‹¤.

### Output Mode

ì¶œë ¥ ëª¨ë“œëŠ” ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ì˜ ê²°ê³¼ë¥¼ `sink` ì— ì–´ë–»ê²Œ ì“¸ì§€ë¥¼ ê²°ì •í•œë‹¤.

`Complete` ëª¨ë“œëŠ” ë§¤ íŠ¸ë¦¬ê±°ë§ˆë‹¤ ì „ì²´ ì§‘ê³„ ê²°ê³¼ í…Œì´ë¸”ì„ ì¶œë ¥í•œë‹¤. ì „ì²´ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤˜ì•¼ í•˜ë¯€ë¡œ ì›Œí„°ë§ˆí¬ì— ì˜í•´ ì˜¤ëž˜ëœ ìœˆë„ìš° ìƒíƒœë¥¼ ì •ë¦¬í•  ìˆ˜ ì—†ë‹¤. ì¦‰, Complete ëª¨ë“œì—ì„œ ì›Œí„°ë§ˆí¬ëŠ” ëŠ¦ê²Œ ì˜¨ ë°ì´í„° í•„í„°ë§ ì—­í• ë§Œ í•˜ì§€, ìƒíƒœ ì •ë¦¬ ì—­í• ì€ ìˆ˜í–‰í•  ìˆ˜ ì—†ë‹¤. ì´ëŠ” ê²°êµ­ OOM ì˜¤ë¥˜ë¥¼ ìœ ë°œí•˜ê²Œ ë˜ë¯€ë¡œ, ìž¥ì‹œê°„ ì‹¤í–‰ë˜ëŠ” Stateful ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” ì‚¬ìš©í•˜ë©´ ì•ˆ ëœë‹¤.

`Update` ëª¨ë“œëŠ” ë§ˆì§€ë§‰ íŠ¸ë¦¬ê±° ì´í›„ ê²°ê³¼ê°€ ë³€ê²½ëœ í–‰ë§Œ ì¶œë ¥í•œë‹¤. ë°ì´í„°ê°€ ë“¤ì–´ì™€ ìœˆë„ìš°ì˜ ì§‘ê³„ ê°’ì´ ì—…ë°ì´íŠ¸ë˜ë©´ í•´ë‹¹ ìœˆë„ìš°ì˜ ë³€ê²½ëœ ê²°ê³¼ê°€ ì‹±í¬ë¡œ ì¶œë ¥ëœë‹¤. ì›Œí„°ë§ˆí¬ë¡œ ì§„í–‰ë˜ì–´ íŠ¹ì • ìœˆë„ìš°ê°€ ë§Œë£Œë˜ë©´ í•´ë‹¹ ìœˆë„ìš°ì˜ ìƒíƒœë¥¼ ì‚­ì œí•œë‹¤. Update ëª¨ë“œëŠ” ê¸°ì¡´ ê²°ê³¼ í–‰ì„ ìˆ˜ì •í•´ì•¼ í•˜ë¯€ë¡œ ì‹±í¬ëŠ” í‚¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–‰ë¥¼ ì°¾ì•„ ìˆ˜ì •í•´ì•¼ í•œë‹¤. ì´ë¥¼ `Upsert` ë¼ê³  í•œë‹¤. ë”°ë¼ì„œ í‚¤ ê¸°ë°˜ ìˆ˜ì •ì´ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì— ì í•©í•˜ë‹¤.

`Append` ëª¨ë“œëŠ” í™•ì •ë˜ì–´ ë‹¤ì‹œëŠ” ë³€ê²½ë˜ì§€ ì•Šì„ í–‰ë§Œ ì¶œë ¥í•œë‹¤. ì›Œí„°ë§ˆí¬ê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ì ì¸ ì§‘ê³„ ì—°ì‚°ì€ ê³„ì‚°ì´ ê³„ì† ë°”ë€” ìˆ˜ ìžˆìœ¼ë¯€ë¡œ Append ëª¨ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. Append ëª¨ë“œì—ì„œ ì›Œí„°ë§ˆí¬ëŠ” íŠ¹ì • ìœˆë„ìš°ë¥¼ ìµœì¢… í™•ì •ì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ê°€ì ¸ì˜¨ë‹¤. ì›ë„ìš°ì˜ ì¢…ë£Œ ì‹œê°„ì´ ì›Œí„°ë§ˆí¬ë¥¼ ì§€ë‚˜ë©´ í•´ë‹¹ ìœˆë„ìš°ì˜ ê²°ê³¼ëŠ” ë” ì´ìƒ ë³€ê²½ë˜ì§€ ì•Šì„ ê²ƒì´ë¼ê³  ë³´ìž¥í•  ìˆ˜ ìžˆë‹¤. ë‹¨, ì›ë„ìš°ì˜ ê²°ê³¼ëŠ” ì›Œí„°ë§ˆí¬ì— ì˜í•´ ë§Œë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì•¼ ë³¼ ìˆ˜ ìžˆë‹¤.

---

```python
window_duration = "5 minutes"
window_agg_df = tf_df \
    .withWatermark("create_date", "10 minutes") \
    .groupBy(window(col("create_date"), window_duration)) \
    .sum("amount").withColumnRenamed("sum(amount)", "total_amount")
```

`withWatermark` ë©”ì„œë“œë¥¼ í†µí•´ ì›Œí„°ë§ˆí¬ë¥¼ ì •ì˜í•œë‹¤. ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ì— ì›Œí„°ë§ˆí¬ ê³„ì‚° ì‹œ ì‚¬ìš©í•  ê¸°ì¤€ ì‹œê°„ ì»¬ëŸ¼, ë‘ ë²ˆì§¸ íŒŒë¼ë¯¸í„°ì— ì§€ì—° ìž„ê³„ê°’ì„ ì„¤ì •í•œë‹¤.

## ðŸ“Œ Streaming - Static Join w/ Cassandra

### Cassandra

`Cassandra` ëŠ” ìˆ˜ë§Žì€ ë°ì´í„°ë¥¼ ì§€ì—° ì—†ì´ ì €ìž¥í•˜ê³ , ì‚¬ìš©ìž ìˆ˜ê°€ ëŠ˜ì–´ë‚  ë•Œë§ˆë‹¤ ì„œë²„ë¥¼ ìœ ì—°í•˜ê²Œ ì¶”ê°€í•  ìˆ˜ ìžˆê³ , íŠ¹ì • ì„œë²„ì— ìž¥ì• ê°€ ë°œìƒí•´ë„ ì„œë¹„ìŠ¤ê°€ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ëª©í‘œë¥¼ ê°€ì§€ê³  ë“±ìž¥í•œ ë¶„ì‚° ë°ì´í„°ë² ì´ìŠ¤ì´ë‹¤.

ì¹´ì‚°ë“œë¼ í´ëŸ¬ìŠ¤í„°ì—ëŠ” íŠ¹ë³„í•œ ì—­í• ì„ í•˜ëŠ” ë§ˆìŠ¤í„° ë…¸ë“œê°€ ì¡´ìž¬í•˜ì§€ ì•Šê³ , ëª¨ë“  ë…¸ë“œê°€ ë™ë“±í•œ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. ë…¸ë“œë“¤ì€ `Gossip` ì´ë¼ëŠ” í”„ë¡œí† ì½œì„ í†µí•´ ì„œë¡œì˜ ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ êµí™˜í•œë‹¤. ë§ˆìŠ¤í„° ë…¸ë“œê°€ ì—†ìœ¼ë¯€ë¡œ ì–´ë–¤ ë…¸ë“œê°€ ë‹¤ìš´ë˜ì–´ë„ ì „ì²´ í´ëŸ¬ìŠ¤í„°ëŠ” ë©ˆì¶”ì§€ ì•ŠëŠ”ë‹¤.

ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ì¹´ì‚°ë“œë¼ëŠ” í•´ë‹¹ ë°ì´í„°ë¥¼ ì €ìž¥í•  ë…¸ë“œë“¤ì„ ê²°ì •í•˜ê³  í•´ë‹¹ ë…¸ë“œë“¤ê³¼ ë…¸ë“œì˜ ë³µì œë³¸ì—ê²Œ ë°ì´í„°ë¥¼ ì „ì†¡í•œë‹¤. ê³ ê°€ìš©ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì½ê¸° ë° ì“°ê¸° ìš”ì²­ ì‹œ ëª‡ ê°œì˜ ë…¸ë“œë¡œë¶€í„° ì‘ë‹µì„ ë°›ì•„ì•¼ ìš”ì²­ì´ ì„±ê³µë˜ì—ˆë‹¤ê³  ê°„ì£¼í•œë‹¤.

ì¹´ì‚°ë“œë¼ëŠ” ìˆ˜í‰ì  í™•ìž¥ì— ìµœì í™”ë˜ì–´ ìžˆë‹¤. ë°ì´í„°ë¥¼ ë¶„ì‚°ì‹œí‚¤ê¸° ìœ„í•´ `Hash Ring` ì´ë¼ëŠ” ê°€ìƒì˜ ë§ì„ ì‚¬ìš©í•œë‹¤. ê° ë…¸ë“œëŠ” ë§ì˜ íŠ¹ì • êµ¬ê°„ì„ ë‹´ë‹¹í•˜ë©°, ë°ì´í„°ë¥¼ ì €ìž¥í•  ë•Œ ë°ì´í„°ì˜ PKë¥¼ í•´ì‹±í•˜ì—¬ ë§ ìœ„ì˜ íŠ¹ì • ìœ„ì¹˜ ê°’ì„ ì–»ê³ , ë°ì´í„°ëŠ” í•´ë‹¹ ìœ„ì¹˜ê°€ ì†í•œ êµ¬ê°„ì„ ë‹´ë‹¹í•˜ëŠ” ë…¸ë“œì— ì €ìž¥ëœë‹¤. ì´ ê³¼ì •ì€ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ì´ ì´ë£¨ì–´ì§„ë‹¤.

ë˜í•œ ì¡°ì¸ ì—°ì‚°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤. ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ê²ƒì¸ì§€ ì„¤ê³„í•˜ê³ , ì˜ë„ì ìœ¼ë¡œ ë¹„ì •ê·œí™”í•˜ì—¬ ì¡°íšŒ ì¿¼ë¦¬ì— ìµœì í™”ëœ í…Œì´ë¸”ì„ ë§Œë“ ë‹¤.

`Cassandra Query Language, CQL` ì´ë¼ëŠ” ì¿¼ë¦¬ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, SQLê³¼ ìœ ì‚¬í•œ ë¬¸ë²•ì„ ì‚¬ìš©í•œë‹¤. ê°€ìž¥ í° ì°¨ì´ì ì€ `WHERE` ì ˆì˜ ì‚¬ìš©ì´ ì œí•œì ì´ë¼ëŠ” ì ì´ë‹¤. ì„±ëŠ¥ì„ ë³´ìž¥í•˜ê¸° ìœ„í•´ PKì— í¬í•¨ëœ ì»¬ëŸ¼ì„ ì¡°ê±´ìœ¼ë¡œë§Œ ì¡°íšŒí•  ìˆ˜ ìžˆë‹¤.

---

```python
spark = SparkSession \
    .builder \
    .appName("WaterMark") \
    .config("spark.streaming.stopGracefullyOnShutdown", "true") \
    .config("spark.sql.shuffle.partitions", "3") \
    .config("spark.cassandra.connection.host", "cassandra") \
    .config("spark.cassandra.connection.port", "9042") \
    .config("spark.cassandra.auth.username", "cassandra") \
    .config("spark.cassandra.auth.password", "cassandra") \
    .config("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions") \
    .config("spark.sql.catalog.lh", "com.datastax.spark.connector.datasource.CassandraCatalog") \
    .getOrCreate()
```

`spark-cassandra-connector` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŒŒí¬ì™€ ì¹´ì‚°ë“œë¼ í´ëŸ¬ìŠ¤í„°ë¥¼ ì—°ê²°í•œë‹¤.

```python
join_df = event_df.join(
            user_df,
            event_df.login_id == user_df.login_id,
            "inner").drop(user_df.login_id)
```

ìŠ¤íŒŒí¬ëŠ” ì¹´í”„ì¹´ë¡œë¶€í„° ë§ˆì´í¬ë¡œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì´ë²¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ë‹¤. ê° ë§ˆì´í¬ë¡œ ë°°ì¹˜ì— ëŒ€í•´ ë©”ëª¨ë¦¬ì— ë¡œë“œí•´ ì¤€ ì •ì  ë°ì´í„°ì™€ ì¡°ì¸ì„ ìˆ˜í–‰í•œë‹¤. ìµœì¢… ê²°ê³¼ëŠ” ì •ì  ë°ì´í„°ê°€ í¬í•¨ëœ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°í”„ë ˆìž„ì´ ëœë‹¤. ì´ì „ ìƒíƒœë¥¼ ê¸°ì–µí•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ Stateless transformationì´ë‹¤.

```python
def cassandraWriter(batch_df, batch_id):
    batch_df \
        .write \
        .format("org.apache.spark.sql.cassandra") \
        .option("keyspace", "test_db") \
        .option("table", "users") \
        .mode("append") \
        .save()
    
query = output_df \
        .writeStream \
        .foreachBatch(cassandraWriter) \
        .outputMode("update") \
        .trigger(processingTime="5 seconds") \
        .start()
```

`foreachBatch` ëŠ” ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°í”„ë ˆìž„ì„ ë§ˆì´í¬ë¡œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•  ë•Œ ê° ë°°ì¹˜ë¥¼ ì¼ë°˜ì ì¸ ì •ì  ë°ì´í„°í”„ë ˆìž„ì²˜ëŸ¼ ë‹¤ë£° ìˆ˜ ìžˆê²Œ í•˜ëŠ” ë©”ì„œë“œì´ë‹¤.

## ðŸ“Œ Streaming - Streaming Join

```python
join_df = impressions_df.join(
    clicks_df,
    impressions_df.uuid == clicks_df.uuid,
    "inner"
)
```

ìŠ¤íŒŒí¬ëŠ” ì–‘ìª½ ìŠ¤íŠ¸ë¦¼ ê°ê°ì— ëŒ€í•´ ìƒíƒœ ì €ìž¥ì†Œë¥¼ ìœ ì§€í•œë‹¤. ì¡°ì¸ ì¡°ê±´ì´ ì¼ì¹˜í•˜ë©´ ë‘ ë°ì´í„°ë¥¼ í•©ì¹œ ìƒˆë¡œìš´ í–‰ì„ ë§Œë“¤ì–´ ê²°ê³¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë‚´ë³´ë‚¸ë‹¤.

inner joinì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì¡°ì¸ ì¡°ê±´ì´ ì¼ì¹˜í•˜ëŠ” ìˆœê°„ ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ë§Œ, outer joinì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì–¸ì œê¹Œì§€ ë°ì´í„°ë¥¼ ê¸°ë‹¤ë¦¬ê³ , í•´ë‹¹ í•„ë“œë¥¼ nullë¡œ ì„¤ì •í• ì§€ì— ëŒ€í•œ ê¸°ì¤€ì´ ìžˆì–´ì•¼ í•œë‹¤.

```python
impressions_df = impression_events.select(...) \
    .withColumnRenamed("create_date", "impr_date") \
    .withWatermark("impr_date", "5 minutes") \
    .withColumnRenamed("uuid", "impr_uuid")

clicks_df = click_events.select(...) \
    .withColumnRenamed("create_date", "click_date") \
    .withWatermark("click_date", "5 minutes") \
    .withColumnRenamed("uuid", "click_uuid")

```

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì–‘ìª½ ìŠ¤íŠ¸ë¦¼ì— ì›Œí„°ë§ˆí¬ë¥¼ ì„¤ì •í•œë‹¤.

```python
join_df = impressions_df.join(
    clicks_df,
    expr("""
        impr_uuid = click_uuid AND
        click_date >= impr_date AND
        click_date <= impr_date + interval 5 minutes
        """),
    "leftOuter"
)
```

ê·¸ë¦¬ê³  `expr` ë©”ì„œë“œë¥¼ í†µí•´ ëª…ì‹œì ìœ¼ë¡œ ì¡°ì¸ ì¡°ê±´ì„ ì„¸ë¶€ì ìœ¼ë¡œ ì„¤ì •í•œë‹¤.

ì •ë¦¬í•˜ë©´, ì›Œí„°ë§ˆí¬ë¥¼ í†µí•´ ì–¸ì œ ìƒíƒœë¥¼ ì •ë¦¬í•´ì•¼ í• ì§€ ê²°ì •í•  ìˆ˜ ìžˆìœ¼ë©°, `expr` ë¥¼ í†µí•œ ì„¸ë¶€ ì¡°ê±´ì„ í†µí•´ ì–¸ì œê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì¡°ì¸í• ì§€ ê²°ì •í•  ìˆ˜ ìžˆë‹¤.

## ðŸ“Œ ì°¸ê³ 

https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html